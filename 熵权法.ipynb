{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 熵权法"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"SimHei\"]  #设置字体\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False  #该语句解决图像中的“-”负号的乱码问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "       var1     var2  var3    var4     var5  var6\n0    171.33   151.33  0.28    0.00   106.36  0.05\n1    646.66   370.00  1.07   61.00  1686.79  1.64\n2    533.33   189.66  0.59    0.00   242.31  0.57\n3     28.33     0.00  0.17    0.00   137.85  2.29\n4    620.00   234.00  0.88   41.33   428.33  0.13\n5    192.33   177.66  0.16    0.00   128.68  1.07\n6    111.00    94.00  0.18    0.00   234.27  0.22\n7    291.00   654.00  1.21   65.66     2.26  0.00\n8    421.33   247.00  0.70    0.00     0.40  0.00\n9    193.00   288.66  0.16    0.00     0.00  0.00\n10    82.33   118.00  0.11    0.00   758.41  0.24\n11   649.66   648.66  0.54    0.00    13.35  0.11\n12    37.66   103.33  0.12    0.00  1133.51  1.10\n13   183.33   282.33  0.55    0.00   624.73  1.04\n14  1014.66  1264.66  5.07  814.66     0.00  0.00\n15    90.66   134.00  0.30    0.00     0.15  0.00\n16   200.66    98.33  0.33    0.00   681.54  0.51\n17   540.66   558.66  1.08   62.00     2.71  0.09\n18    80.00    60.66  0.13    0.00   910.19  0.88\n19   530.66   281.33  0.88   36.00   743.21  0.72\n20   166.00   133.00  0.13    0.00   246.88  2.05\n21   377.66   310.33  0.57    0.00   102.89  0.57\n22   143.33    73.00  0.23    0.00   103.94  0.10\n23   394.66   473.66  0.56    0.00     1.06  0.03\n24   535.66   447.33  0.44    0.00    10.59  0.08\n25    52.66    56.66  0.52    0.00     0.00  0.00\n26  1381.66   760.66  2.30  781.66   248.71  0.13\n27    44.33    42.33  0.07    0.00     0.66  0.00\n28    71.66    62.66  0.11    0.00   535.26  0.52\n29   148.33    56.66  0.24    0.00   173.83  0.16",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>var1</th>\n      <th>var2</th>\n      <th>var3</th>\n      <th>var4</th>\n      <th>var5</th>\n      <th>var6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>171.33</td>\n      <td>151.33</td>\n      <td>0.28</td>\n      <td>0.00</td>\n      <td>106.36</td>\n      <td>0.05</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>646.66</td>\n      <td>370.00</td>\n      <td>1.07</td>\n      <td>61.00</td>\n      <td>1686.79</td>\n      <td>1.64</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>533.33</td>\n      <td>189.66</td>\n      <td>0.59</td>\n      <td>0.00</td>\n      <td>242.31</td>\n      <td>0.57</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>28.33</td>\n      <td>0.00</td>\n      <td>0.17</td>\n      <td>0.00</td>\n      <td>137.85</td>\n      <td>2.29</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>620.00</td>\n      <td>234.00</td>\n      <td>0.88</td>\n      <td>41.33</td>\n      <td>428.33</td>\n      <td>0.13</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>192.33</td>\n      <td>177.66</td>\n      <td>0.16</td>\n      <td>0.00</td>\n      <td>128.68</td>\n      <td>1.07</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>111.00</td>\n      <td>94.00</td>\n      <td>0.18</td>\n      <td>0.00</td>\n      <td>234.27</td>\n      <td>0.22</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>291.00</td>\n      <td>654.00</td>\n      <td>1.21</td>\n      <td>65.66</td>\n      <td>2.26</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>421.33</td>\n      <td>247.00</td>\n      <td>0.70</td>\n      <td>0.00</td>\n      <td>0.40</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>193.00</td>\n      <td>288.66</td>\n      <td>0.16</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>82.33</td>\n      <td>118.00</td>\n      <td>0.11</td>\n      <td>0.00</td>\n      <td>758.41</td>\n      <td>0.24</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>649.66</td>\n      <td>648.66</td>\n      <td>0.54</td>\n      <td>0.00</td>\n      <td>13.35</td>\n      <td>0.11</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>37.66</td>\n      <td>103.33</td>\n      <td>0.12</td>\n      <td>0.00</td>\n      <td>1133.51</td>\n      <td>1.10</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>183.33</td>\n      <td>282.33</td>\n      <td>0.55</td>\n      <td>0.00</td>\n      <td>624.73</td>\n      <td>1.04</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1014.66</td>\n      <td>1264.66</td>\n      <td>5.07</td>\n      <td>814.66</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>90.66</td>\n      <td>134.00</td>\n      <td>0.30</td>\n      <td>0.00</td>\n      <td>0.15</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>200.66</td>\n      <td>98.33</td>\n      <td>0.33</td>\n      <td>0.00</td>\n      <td>681.54</td>\n      <td>0.51</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>540.66</td>\n      <td>558.66</td>\n      <td>1.08</td>\n      <td>62.00</td>\n      <td>2.71</td>\n      <td>0.09</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>80.00</td>\n      <td>60.66</td>\n      <td>0.13</td>\n      <td>0.00</td>\n      <td>910.19</td>\n      <td>0.88</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>530.66</td>\n      <td>281.33</td>\n      <td>0.88</td>\n      <td>36.00</td>\n      <td>743.21</td>\n      <td>0.72</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>166.00</td>\n      <td>133.00</td>\n      <td>0.13</td>\n      <td>0.00</td>\n      <td>246.88</td>\n      <td>2.05</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>377.66</td>\n      <td>310.33</td>\n      <td>0.57</td>\n      <td>0.00</td>\n      <td>102.89</td>\n      <td>0.57</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>143.33</td>\n      <td>73.00</td>\n      <td>0.23</td>\n      <td>0.00</td>\n      <td>103.94</td>\n      <td>0.10</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>394.66</td>\n      <td>473.66</td>\n      <td>0.56</td>\n      <td>0.00</td>\n      <td>1.06</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>535.66</td>\n      <td>447.33</td>\n      <td>0.44</td>\n      <td>0.00</td>\n      <td>10.59</td>\n      <td>0.08</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>52.66</td>\n      <td>56.66</td>\n      <td>0.52</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>1381.66</td>\n      <td>760.66</td>\n      <td>2.30</td>\n      <td>781.66</td>\n      <td>248.71</td>\n      <td>0.13</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>44.33</td>\n      <td>42.33</td>\n      <td>0.07</td>\n      <td>0.00</td>\n      <td>0.66</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>71.66</td>\n      <td>62.66</td>\n      <td>0.11</td>\n      <td>0.00</td>\n      <td>535.26</td>\n      <td>0.52</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>148.33</td>\n      <td>56.66</td>\n      <td>0.24</td>\n      <td>0.00</td>\n      <td>173.83</td>\n      <td>0.16</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('topsis.csv')\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "第一步：数据标准化。类似`sklearn.preprocess.Standard`的方法。之后计算出每一项所占的比重"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "        var1      var2      var3      var4      var5      var6\n0   0.015741  0.018075  0.011945  0.000000  0.011487  0.003497\n1   0.068063  0.044192  0.056883  0.032755  0.182181  0.114685\n2   0.055588  0.022653  0.029579  0.000000  0.026171  0.039860\n3   0.000000  0.000000  0.005688  0.000000  0.014888  0.160140\n4   0.065129  0.027948  0.046075  0.022193  0.046262  0.009091\n5   0.018052  0.021219  0.005119  0.000000  0.013898  0.074825\n6   0.009100  0.011227  0.006257  0.000000  0.025302  0.015385\n7   0.028914  0.078112  0.064846  0.035257  0.000244  0.000000\n8   0.043260  0.029501  0.035836  0.000000  0.000043  0.000000\n9   0.018126  0.034477  0.005119  0.000000  0.000000  0.000000\n10  0.005944  0.014094  0.002275  0.000000  0.081912  0.016783\n11  0.068393  0.077475  0.026735  0.000000  0.001442  0.007692\n12  0.001027  0.012342  0.002844  0.000000  0.122424  0.076923\n13  0.017062  0.033721  0.027304  0.000000  0.067474  0.072727\n14  0.108571  0.151048  0.284414  0.437446  0.000000  0.000000\n15  0.006861  0.016005  0.013083  0.000000  0.000016  0.000000\n16  0.018969  0.011744  0.014790  0.000000  0.073609  0.035664\n17  0.056395  0.066725  0.057452  0.033292  0.000293  0.006294\n18  0.005688  0.007245  0.003413  0.000000  0.098305  0.061538\n19  0.055294  0.033601  0.046075  0.019331  0.080270  0.050350\n20  0.015154  0.015885  0.003413  0.000000  0.026664  0.143357\n21  0.038453  0.037065  0.028441  0.000000  0.011113  0.039860\n22  0.012659  0.008719  0.009101  0.000000  0.011226  0.006993\n23  0.040324  0.056573  0.027873  0.000000  0.000114  0.002098\n24  0.055845  0.053428  0.021047  0.000000  0.001144  0.005594\n25  0.002678  0.006767  0.025597  0.000000  0.000000  0.000000\n26  0.148969  0.090852  0.126849  0.419726  0.026862  0.009091\n27  0.001761  0.005056  0.000000  0.000000  0.000071  0.000000\n28  0.004770  0.007484  0.002275  0.000000  0.057811  0.036364\n29  0.013209  0.006767  0.009670  0.000000  0.018774  0.011189",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>var1</th>\n      <th>var2</th>\n      <th>var3</th>\n      <th>var4</th>\n      <th>var5</th>\n      <th>var6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.015741</td>\n      <td>0.018075</td>\n      <td>0.011945</td>\n      <td>0.000000</td>\n      <td>0.011487</td>\n      <td>0.003497</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.068063</td>\n      <td>0.044192</td>\n      <td>0.056883</td>\n      <td>0.032755</td>\n      <td>0.182181</td>\n      <td>0.114685</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.055588</td>\n      <td>0.022653</td>\n      <td>0.029579</td>\n      <td>0.000000</td>\n      <td>0.026171</td>\n      <td>0.039860</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.005688</td>\n      <td>0.000000</td>\n      <td>0.014888</td>\n      <td>0.160140</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.065129</td>\n      <td>0.027948</td>\n      <td>0.046075</td>\n      <td>0.022193</td>\n      <td>0.046262</td>\n      <td>0.009091</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.018052</td>\n      <td>0.021219</td>\n      <td>0.005119</td>\n      <td>0.000000</td>\n      <td>0.013898</td>\n      <td>0.074825</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.009100</td>\n      <td>0.011227</td>\n      <td>0.006257</td>\n      <td>0.000000</td>\n      <td>0.025302</td>\n      <td>0.015385</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.028914</td>\n      <td>0.078112</td>\n      <td>0.064846</td>\n      <td>0.035257</td>\n      <td>0.000244</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.043260</td>\n      <td>0.029501</td>\n      <td>0.035836</td>\n      <td>0.000000</td>\n      <td>0.000043</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.018126</td>\n      <td>0.034477</td>\n      <td>0.005119</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.005944</td>\n      <td>0.014094</td>\n      <td>0.002275</td>\n      <td>0.000000</td>\n      <td>0.081912</td>\n      <td>0.016783</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.068393</td>\n      <td>0.077475</td>\n      <td>0.026735</td>\n      <td>0.000000</td>\n      <td>0.001442</td>\n      <td>0.007692</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.001027</td>\n      <td>0.012342</td>\n      <td>0.002844</td>\n      <td>0.000000</td>\n      <td>0.122424</td>\n      <td>0.076923</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.017062</td>\n      <td>0.033721</td>\n      <td>0.027304</td>\n      <td>0.000000</td>\n      <td>0.067474</td>\n      <td>0.072727</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.108571</td>\n      <td>0.151048</td>\n      <td>0.284414</td>\n      <td>0.437446</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.006861</td>\n      <td>0.016005</td>\n      <td>0.013083</td>\n      <td>0.000000</td>\n      <td>0.000016</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.018969</td>\n      <td>0.011744</td>\n      <td>0.014790</td>\n      <td>0.000000</td>\n      <td>0.073609</td>\n      <td>0.035664</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.056395</td>\n      <td>0.066725</td>\n      <td>0.057452</td>\n      <td>0.033292</td>\n      <td>0.000293</td>\n      <td>0.006294</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.005688</td>\n      <td>0.007245</td>\n      <td>0.003413</td>\n      <td>0.000000</td>\n      <td>0.098305</td>\n      <td>0.061538</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.055294</td>\n      <td>0.033601</td>\n      <td>0.046075</td>\n      <td>0.019331</td>\n      <td>0.080270</td>\n      <td>0.050350</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.015154</td>\n      <td>0.015885</td>\n      <td>0.003413</td>\n      <td>0.000000</td>\n      <td>0.026664</td>\n      <td>0.143357</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.038453</td>\n      <td>0.037065</td>\n      <td>0.028441</td>\n      <td>0.000000</td>\n      <td>0.011113</td>\n      <td>0.039860</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.012659</td>\n      <td>0.008719</td>\n      <td>0.009101</td>\n      <td>0.000000</td>\n      <td>0.011226</td>\n      <td>0.006993</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.040324</td>\n      <td>0.056573</td>\n      <td>0.027873</td>\n      <td>0.000000</td>\n      <td>0.000114</td>\n      <td>0.002098</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.055845</td>\n      <td>0.053428</td>\n      <td>0.021047</td>\n      <td>0.000000</td>\n      <td>0.001144</td>\n      <td>0.005594</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.002678</td>\n      <td>0.006767</td>\n      <td>0.025597</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.148969</td>\n      <td>0.090852</td>\n      <td>0.126849</td>\n      <td>0.419726</td>\n      <td>0.026862</td>\n      <td>0.009091</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.001761</td>\n      <td>0.005056</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000071</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.004770</td>\n      <td>0.007484</td>\n      <td>0.002275</td>\n      <td>0.000000</td>\n      <td>0.057811</td>\n      <td>0.036364</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.013209</td>\n      <td>0.006767</td>\n      <td>0.009670</td>\n      <td>0.000000</td>\n      <td>0.018774</td>\n      <td>0.011189</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for column in data.columns:\n",
    "    data[column] = data[column].apply(\n",
    "        lambda x: (x - np.min(data[column])) / (np.max(data[column] - np.min(data[column]))))\n",
    "    sum = np.sum(data[column])\n",
    "    data[column] = data[column].apply(lambda x: x / sum)\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "第二步：求各个指标的信息熵。\n",
    "公式如下：\n",
    "$$\n",
    "e_j = -K \\sum_{i=1}^m y_{ij} lny_{ij}, \\quad e_j>0 \\\\\n",
    "K = \\frac{1}{ln m}, \\quad 其中，m为影响因素的个数\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "6"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 1 / np.log(data.columns.size)\n",
    "data.columns.size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.57408549e-02, 1.80745196e-02, 1.19453925e-02, 0.00000000e+00,\n        1.14873629e-02, 3.49650350e-03],\n       [6.80632364e-02, 4.41919795e-02, 5.68828214e-02, 3.27550193e-02,\n        1.82180979e-01, 1.14685315e-01],\n       [5.55883337e-02, 2.26525698e-02, 2.95790671e-02, 0.00000000e+00,\n        2.61705802e-02, 3.98601399e-02],\n       [0.00000000e+00, 0.00000000e+00, 5.68828214e-03, 0.00000000e+00,\n        1.48884259e-02, 1.60139860e-01],\n       [6.51286127e-02, 2.79484411e-02, 4.60750853e-02, 2.21928680e-02,\n        4.62615848e-02, 9.09090909e-03],\n       [1.80524490e-02, 2.12193164e-02, 5.11945392e-03, 0.00000000e+00,\n        1.38980243e-02, 7.48251748e-02],\n       [9.09997534e-03, 1.12271516e-02, 6.25711035e-03, 0.00000000e+00,\n        2.53022237e-02, 1.53846154e-02],\n       [2.89136388e-02, 7.81123097e-02, 6.48464164e-02, 3.52572880e-02,\n        2.44090262e-04, 0.00000000e+00],\n       [4.32598320e-02, 2.95011323e-02, 3.58361775e-02, 0.00000000e+00,\n        4.32018162e-05, 0.00000000e+00],\n       [1.81261998e-02, 3.44769103e-02, 5.11945392e-03, 0.00000000e+00,\n        0.00000000e+00, 0.00000000e+00],\n       [5.94409905e-03, 1.40936583e-02, 2.27531286e-03, 0.00000000e+00,\n        8.19117236e-02, 1.67832168e-02],\n       [6.83934641e-02, 7.74745120e-02, 2.67349261e-02, 0.00000000e+00,\n        1.44186062e-03, 7.69230769e-03],\n       [1.02700822e-03, 1.23415061e-02, 2.84414107e-03, 0.00000000e+00,\n        1.22424227e-01, 7.69230769e-02],\n       [1.70617658e-02, 3.37208691e-02, 2.73037543e-02, 0.00000000e+00,\n        6.74736766e-02, 7.27272727e-02],\n       [1.08571171e-01, 1.51048186e-01, 2.84414107e-01, 4.37445968e-01,\n        0.00000000e+00, 0.00000000e+00],\n       [6.86103137e-03, 1.60046629e-02, 1.30830489e-02, 0.00000000e+00,\n        1.62006811e-05, 0.00000000e+00],\n       [1.89693813e-02, 1.17443172e-02, 1.47895336e-02, 0.00000000e+00,\n        7.36094145e-02, 3.56643357e-02],\n       [5.63951901e-02, 6.67251116e-02, 5.74516496e-02, 3.32919868e-02,\n        2.92692305e-04, 6.29370629e-03],\n       [5.68762218e-03, 7.24509588e-03, 3.41296928e-03, 0.00000000e+00,\n        9.83046527e-02, 6.15384615e-02],\n       [5.52944310e-02, 3.36014313e-02, 4.60750853e-02, 1.93308311e-02,\n        8.02700546e-02, 5.03496503e-02],\n       [1.51541503e-02, 1.58852251e-02, 3.41296928e-03, 0.00000000e+00,\n        2.66641610e-02, 1.43356643e-01],\n       [3.84528171e-02, 3.70651270e-02, 2.84414107e-02, 0.00000000e+00,\n        1.11125872e-02, 3.98601399e-02],\n       [1.26587295e-02, 8.71895812e-03, 9.10125142e-03, 0.00000000e+00,\n        1.12259919e-02, 6.99300699e-03],\n       [4.03241075e-02, 5.65729000e-02, 2.78725825e-02, 0.00000000e+00,\n        1.14484813e-04, 2.09790210e-03],\n       [5.58448106e-02, 5.34281032e-02, 2.10466439e-02, 0.00000000e+00,\n        1.14376808e-03, 5.59440559e-03],\n       [2.67814685e-03, 6.76734475e-03, 2.55972696e-02, 0.00000000e+00,\n        0.00000000e+00, 0.00000000e+00],\n       [1.48969029e-01, 9.08515436e-02, 1.26848692e-01, 4.19726039e-01,\n        2.68618093e-02, 9.09090909e-03],\n       [1.76121453e-03, 5.05580133e-03, 0.00000000e+00, 0.00000000e+00,\n        7.12829967e-05, 0.00000000e+00],\n       [4.76958911e-03, 7.48397145e-03, 2.27531286e-03, 0.00000000e+00,\n        5.78105104e-02, 3.63636364e-02],\n       [1.32091090e-02, 6.76734475e-03, 9.67007964e-03, 0.00000000e+00,\n        1.87744293e-02, 1.11888112e-02]])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array(data)\n",
    "x\n",
    "# x.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.06806324, 0.04419198, 0.05688282, 0.03275502, 0.18218098,\n       0.11468531])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1,:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "li = []\n",
    "for i in range(x.shape[0]):\n",
    "    e_i = -(K*np.sum(x[i,:]))\n",
    "    li.append(e_i)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "其中$e_i$就是计算得出的30项目标的熵值。\n",
    "接下来，计算每一项指标的差异系数。\n",
    "差异系数：\n",
    "$$\n",
    "g_i = 1-e_i\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "li = [1-i for i in li]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "最后，计算各项指标的权重。\n",
    "$$\n",
    "W_i = \\frac{g_i}{ \\sum_{1}^{m} g_i\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "{0: 0.03204436051114899,\n 1: 0.03609870383397827,\n 2: 0.033091290604232194,\n 3: 0.03315484240593499,\n 4: 0.03348788841848215,\n 5: 0.0327142283108847,\n 6: 0.03210477044535342,\n 7: 0.03340158603222835,\n 8: 0.03248769185378638,\n 9: 0.03201638769108399,\n 10: 0.03260216921356548,\n 11: 0.033164288359362075,\n 12: 0.03347735912388226,\n 13: 0.03350260424119412,\n 14: 0.04056684808243422,\n 15: 0.031814995228932004,\n 16: 0.03291474091340054,\n 17: 0.03352262534322613,\n 18: 0.033112932580859525,\n 19: 0.034119381922857885,\n 20: 0.033374737608058085,\n 21: 0.032916176543524606,\n 22: 0.0319328541220345,\n 23: 0.032657465324992176,\n 24: 0.032750728327984126,\n 25: 0.0318064593403426,\n 26: 0.03909389901859041,\n 27: 0.0315458565520355,\n 28: 0.03248827199962576,\n 29: 0.03203385604598457}"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_weight = np.sum(li)\n",
    "weight = [x/sum_weight for x in li]\n",
    "weight_dic = dict(list(enumerate(weight)))\n",
    "weight_dic"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "[(27, 0.0315458565520355),\n (25, 0.0318064593403426),\n (15, 0.031814995228932004),\n (22, 0.0319328541220345),\n (9, 0.03201638769108399),\n (29, 0.03203385604598457),\n (0, 0.03204436051114899),\n (6, 0.03210477044535342),\n (8, 0.03248769185378638),\n (28, 0.03248827199962576),\n (10, 0.03260216921356548),\n (23, 0.032657465324992176),\n (5, 0.0327142283108847),\n (24, 0.032750728327984126),\n (16, 0.03291474091340054),\n (21, 0.032916176543524606),\n (2, 0.033091290604232194),\n (18, 0.033112932580859525),\n (3, 0.03315484240593499),\n (11, 0.033164288359362075),\n (20, 0.033374737608058085),\n (7, 0.03340158603222835),\n (12, 0.03347735912388226),\n (4, 0.03348788841848215),\n (13, 0.03350260424119412),\n (17, 0.03352262534322613),\n (19, 0.034119381922857885),\n (1, 0.03609870383397827),\n (26, 0.03909389901859041),\n (14, 0.04056684808243422)]"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(weight_dic.items(),key=lambda x:x[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "0.04419197951403153"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['var2'][1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "30"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.index.size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from numpy import array\n",
    "\n",
    "# 1读取数据\n",
    "df = pd.read_csv('topsis.csv')\n",
    "# 2数据预处理 ,去除空值的记录\n",
    "df.dropna()\n",
    "\n",
    "\n",
    "#定义熵值法函数\n",
    "def cal_weight(x):\n",
    "    '''熵值法计算变量的权重'''\n",
    "    # 标准化\n",
    "    x = x.apply(lambda x: ((x - np.min(x)) / (np.max(x) - np.min(x))))\n",
    "\n",
    "    # 求k\n",
    "    rows = x.index.size  # 行\n",
    "    cols = x.columns.size  # 列\n",
    "    k = 1.0 / math.log(rows)\n",
    "\n",
    "    lnf = [[None] * cols for i in range(rows)]\n",
    "\n",
    "    # 矩阵计算--\n",
    "    # 信息熵\n",
    "    # p=array(p)\n",
    "    x = array(x)\n",
    "    lnf = [[None] * cols for i in range(rows)]\n",
    "    lnf = array(lnf)\n",
    "    for i in range(0, rows):\n",
    "        for j in range(0, cols):\n",
    "            if x[i][j] == 0:\n",
    "                lnfij = 0.0\n",
    "            else:\n",
    "                p = x[i][j] / x.sum(axis=0)[j]\n",
    "                lnfij = math.log(p) * p * (-k)\n",
    "            lnf[i][j] = lnfij\n",
    "    lnf = pd.DataFrame(lnf)\n",
    "    E = lnf\n",
    "\n",
    "    # 计算冗余度\n",
    "    d = 1 - E.sum(axis=0)\n",
    "    # 计算各指标的权重\n",
    "    w = [[None] * 1 for i in range(cols)]\n",
    "    for j in range(0, cols):\n",
    "        wj = d[j] / sum(d)\n",
    "        w[j] = wj\n",
    "        # 计算各样本的综合得分,用最原始的数据\n",
    "\n",
    "    w = pd.DataFrame(w)\n",
    "    return w\n",
    "\n",
    "\n",
    "w = cal_weight(df)  # 调用cal_weight\n",
    "w.index = df.columns\n",
    "w.columns = ['weight']\n",
    "print(w)\n",
    "print('运行完成!')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**使用PCA主成分分析查看结果**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "       var1     var2  var3    var4     var5  var6\n0    171.33   151.33  0.28    0.00   106.36  0.05\n1    646.66   370.00  1.07   61.00  1686.79  1.64\n2    533.33   189.66  0.59    0.00   242.31  0.57\n3     28.33     0.00  0.17    0.00   137.85  2.29\n4    620.00   234.00  0.88   41.33   428.33  0.13\n5    192.33   177.66  0.16    0.00   128.68  1.07\n6    111.00    94.00  0.18    0.00   234.27  0.22\n7    291.00   654.00  1.21   65.66     2.26  0.00\n8    421.33   247.00  0.70    0.00     0.40  0.00\n9    193.00   288.66  0.16    0.00     0.00  0.00\n10    82.33   118.00  0.11    0.00   758.41  0.24\n11   649.66   648.66  0.54    0.00    13.35  0.11\n12    37.66   103.33  0.12    0.00  1133.51  1.10\n13   183.33   282.33  0.55    0.00   624.73  1.04\n14  1014.66  1264.66  5.07  814.66     0.00  0.00\n15    90.66   134.00  0.30    0.00     0.15  0.00\n16   200.66    98.33  0.33    0.00   681.54  0.51\n17   540.66   558.66  1.08   62.00     2.71  0.09\n18    80.00    60.66  0.13    0.00   910.19  0.88\n19   530.66   281.33  0.88   36.00   743.21  0.72\n20   166.00   133.00  0.13    0.00   246.88  2.05\n21   377.66   310.33  0.57    0.00   102.89  0.57\n22   143.33    73.00  0.23    0.00   103.94  0.10\n23   394.66   473.66  0.56    0.00     1.06  0.03\n24   535.66   447.33  0.44    0.00    10.59  0.08\n25    52.66    56.66  0.52    0.00     0.00  0.00\n26  1381.66   760.66  2.30  781.66   248.71  0.13\n27    44.33    42.33  0.07    0.00     0.66  0.00\n28    71.66    62.66  0.11    0.00   535.26  0.52\n29   148.33    56.66  0.24    0.00   173.83  0.16",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>var1</th>\n      <th>var2</th>\n      <th>var3</th>\n      <th>var4</th>\n      <th>var5</th>\n      <th>var6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>171.33</td>\n      <td>151.33</td>\n      <td>0.28</td>\n      <td>0.00</td>\n      <td>106.36</td>\n      <td>0.05</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>646.66</td>\n      <td>370.00</td>\n      <td>1.07</td>\n      <td>61.00</td>\n      <td>1686.79</td>\n      <td>1.64</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>533.33</td>\n      <td>189.66</td>\n      <td>0.59</td>\n      <td>0.00</td>\n      <td>242.31</td>\n      <td>0.57</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>28.33</td>\n      <td>0.00</td>\n      <td>0.17</td>\n      <td>0.00</td>\n      <td>137.85</td>\n      <td>2.29</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>620.00</td>\n      <td>234.00</td>\n      <td>0.88</td>\n      <td>41.33</td>\n      <td>428.33</td>\n      <td>0.13</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>192.33</td>\n      <td>177.66</td>\n      <td>0.16</td>\n      <td>0.00</td>\n      <td>128.68</td>\n      <td>1.07</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>111.00</td>\n      <td>94.00</td>\n      <td>0.18</td>\n      <td>0.00</td>\n      <td>234.27</td>\n      <td>0.22</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>291.00</td>\n      <td>654.00</td>\n      <td>1.21</td>\n      <td>65.66</td>\n      <td>2.26</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>421.33</td>\n      <td>247.00</td>\n      <td>0.70</td>\n      <td>0.00</td>\n      <td>0.40</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>193.00</td>\n      <td>288.66</td>\n      <td>0.16</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>82.33</td>\n      <td>118.00</td>\n      <td>0.11</td>\n      <td>0.00</td>\n      <td>758.41</td>\n      <td>0.24</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>649.66</td>\n      <td>648.66</td>\n      <td>0.54</td>\n      <td>0.00</td>\n      <td>13.35</td>\n      <td>0.11</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>37.66</td>\n      <td>103.33</td>\n      <td>0.12</td>\n      <td>0.00</td>\n      <td>1133.51</td>\n      <td>1.10</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>183.33</td>\n      <td>282.33</td>\n      <td>0.55</td>\n      <td>0.00</td>\n      <td>624.73</td>\n      <td>1.04</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1014.66</td>\n      <td>1264.66</td>\n      <td>5.07</td>\n      <td>814.66</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>90.66</td>\n      <td>134.00</td>\n      <td>0.30</td>\n      <td>0.00</td>\n      <td>0.15</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>200.66</td>\n      <td>98.33</td>\n      <td>0.33</td>\n      <td>0.00</td>\n      <td>681.54</td>\n      <td>0.51</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>540.66</td>\n      <td>558.66</td>\n      <td>1.08</td>\n      <td>62.00</td>\n      <td>2.71</td>\n      <td>0.09</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>80.00</td>\n      <td>60.66</td>\n      <td>0.13</td>\n      <td>0.00</td>\n      <td>910.19</td>\n      <td>0.88</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>530.66</td>\n      <td>281.33</td>\n      <td>0.88</td>\n      <td>36.00</td>\n      <td>743.21</td>\n      <td>0.72</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>166.00</td>\n      <td>133.00</td>\n      <td>0.13</td>\n      <td>0.00</td>\n      <td>246.88</td>\n      <td>2.05</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>377.66</td>\n      <td>310.33</td>\n      <td>0.57</td>\n      <td>0.00</td>\n      <td>102.89</td>\n      <td>0.57</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>143.33</td>\n      <td>73.00</td>\n      <td>0.23</td>\n      <td>0.00</td>\n      <td>103.94</td>\n      <td>0.10</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>394.66</td>\n      <td>473.66</td>\n      <td>0.56</td>\n      <td>0.00</td>\n      <td>1.06</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>535.66</td>\n      <td>447.33</td>\n      <td>0.44</td>\n      <td>0.00</td>\n      <td>10.59</td>\n      <td>0.08</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>52.66</td>\n      <td>56.66</td>\n      <td>0.52</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>1381.66</td>\n      <td>760.66</td>\n      <td>2.30</td>\n      <td>781.66</td>\n      <td>248.71</td>\n      <td>0.13</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>44.33</td>\n      <td>42.33</td>\n      <td>0.07</td>\n      <td>0.00</td>\n      <td>0.66</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>71.66</td>\n      <td>62.66</td>\n      <td>0.11</td>\n      <td>0.00</td>\n      <td>535.26</td>\n      <td>0.52</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>148.33</td>\n      <td>56.66</td>\n      <td>0.24</td>\n      <td>0.00</td>\n      <td>173.83</td>\n      <td>0.16</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import KernelPCA,PCA\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "(6, 30)"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array(data).T\n",
    "x.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "x_std = sc.fit_transform(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1.34851825,  0.31143692,  1.92895303,  0.0044562 ,  1.6974762 ,\n         1.28083244,  0.44139176,  0.50846021,  1.87451844,  0.96470121,\n        -0.28542013,  1.41569074, -0.42316005,  0.00591501,  0.93809845,\n         0.97760041,  0.15274958,  1.37311093, -0.28865815,  0.92650054,\n         0.77182414,  1.58039574,  1.57506145,  1.21350535,  1.59596715,\n         1.33502942,  1.71630549,  1.46269832, -0.20877823,  1.17558519],\n       [ 1.07819737, -0.15313248,  0.14811394, -0.56081554,  0.05621805,\n         1.10846993,  0.24247001,  2.02160645,  0.81955151,  1.78356587,\n        -0.15408428,  1.41240562, -0.26432991,  0.44510435,  1.40888802,\n         1.7748882 , -0.26858645,  1.4444496 , -0.34723174,  0.0554214 ,\n         0.43065974,  1.14723817,  0.34282364,  1.59748758,  1.21493831,\n         1.49047628,  0.46603095,  1.36441521, -0.25570446, -0.09036235],\n       [-0.9634011 , -0.77264231, -0.83161442, -0.55742351, -0.93499982,\n        -0.97703421, -0.85534402, -0.69951403, -0.67094558, -0.68603955,\n        -0.58815166, -0.71674805, -0.51395468, -0.80494387, -0.96311934,\n        -0.68467288, -0.67209402, -0.7653847 , -0.53055435, -0.92438104,\n        -0.94299218, -0.84555686, -0.93216492, -0.7020313 , -0.71280963,\n        -0.69122036, -1.06079386, -0.71230681, -0.58184169, -0.86951322],\n       [-0.96718559, -0.67200743, -0.8346717 , -0.56081554, -0.76300787,\n        -0.9789141 , -0.85745025, -0.43085762, -0.67518166, -0.68740917,\n        -0.58855668, -0.71852202, -0.51424491, -0.80738381,  0.56146679,\n        -0.69019172, -0.67345277, -0.52394292, -0.53094807, -0.80168301,\n        -0.94433616, -0.84922387, -0.93619471, -0.7047532 , -0.71470766,\n        -0.71142845,  0.50831076, -0.71574672, -0.58241524, -0.87282758],\n       [ 0.47038086,  2.05803046,  0.42093722,  2.18972131,  0.88250224,\n         0.53298828,  1.88380847, -0.69513716, -0.67276104, -0.68740917,\n         2.20388577, -0.67466563,  2.22727398,  1.96407842, -0.97266696,\n        -0.6874323 ,  2.13273655, -0.75892458,  2.22567518,  1.66908214,\n         1.60798706, -0.18729631,  0.88491717, -0.69960104, -0.66902561,\n        -0.71142845, -0.56469056, -0.68331329,  2.20844356,  1.52773597],\n       [-0.96650979, -0.77168516, -0.83171806, -0.51512291, -0.93818879,\n        -0.96634233, -0.85487597, -0.70455785, -0.67518166, -0.68740917,\n        -0.58767301, -0.71816065, -0.51158444, -0.8027701 , -0.97266696,\n        -0.69019172, -0.67135289, -0.76930832, -0.52828288, -0.92494003,\n        -0.92314261, -0.84555686, -0.93444263, -0.70460739, -0.71436256,\n        -0.71142845, -1.06516278, -0.71574672, -0.57970394, -0.87061801]])"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_std"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "pca_model = PCA(n_components=5)\n",
    "x_pca = pca_model.fit_transform(x_std)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 5.74395233e+00, -1.91449022e+00, -1.78866672e+00,\n        -1.91069723e-01, -9.89359886e-06],\n       [ 4.13228825e+00, -3.10486483e+00,  1.93859932e+00,\n         6.06503280e-02, -3.96961530e-05],\n       [-4.21271139e+00, -5.76275075e-01, -3.26323097e-02,\n        -6.61870793e-01,  2.34193793e-02],\n       [-3.55993803e+00, -1.07723478e+00, -3.81174951e-01,\n         1.38096004e+00, -6.11927111e-05],\n       [ 2.11188579e+00,  7.22159886e+00,  2.97294961e-01,\n         7.79484444e-02,  6.40480399e-05],\n       [-4.21547695e+00, -5.48733961e-01, -3.34202970e-02,\n        -6.66618292e-01, -2.33726449e-02]])"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pca"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**注意到主成分分析与熵权法的差异：主成分分析不会给出原数据各项指标的重要程度，仅仅会直接给出处理后的topK项重要特征。这些特征，具备相关性低（可以通过协方差矩阵或者是数据的相关性计算得出）、代表性高的特点。\n",
    "但是熵权法从信息论的角度出发，可以给出每一项指标的重要程度。**\n",
    "接下来尝试一下随机森林方法。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "data = pd.read_csv('topsis.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "data_arr = np.array(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.71330e+02, 1.51330e+02, 2.80000e-01, 0.00000e+00, 1.06360e+02,\n        5.00000e-02],\n       [6.46660e+02, 3.70000e+02, 1.07000e+00, 6.10000e+01, 1.68679e+03,\n        1.64000e+00],\n       [5.33330e+02, 1.89660e+02, 5.90000e-01, 0.00000e+00, 2.42310e+02,\n        5.70000e-01],\n       [2.83300e+01, 0.00000e+00, 1.70000e-01, 0.00000e+00, 1.37850e+02,\n        2.29000e+00],\n       [6.20000e+02, 2.34000e+02, 8.80000e-01, 4.13300e+01, 4.28330e+02,\n        1.30000e-01],\n       [1.92330e+02, 1.77660e+02, 1.60000e-01, 0.00000e+00, 1.28680e+02,\n        1.07000e+00],\n       [1.11000e+02, 9.40000e+01, 1.80000e-01, 0.00000e+00, 2.34270e+02,\n        2.20000e-01],\n       [2.91000e+02, 6.54000e+02, 1.21000e+00, 6.56600e+01, 2.26000e+00,\n        0.00000e+00],\n       [4.21330e+02, 2.47000e+02, 7.00000e-01, 0.00000e+00, 4.00000e-01,\n        0.00000e+00],\n       [1.93000e+02, 2.88660e+02, 1.60000e-01, 0.00000e+00, 0.00000e+00,\n        0.00000e+00],\n       [8.23300e+01, 1.18000e+02, 1.10000e-01, 0.00000e+00, 7.58410e+02,\n        2.40000e-01],\n       [6.49660e+02, 6.48660e+02, 5.40000e-01, 0.00000e+00, 1.33500e+01,\n        1.10000e-01],\n       [3.76600e+01, 1.03330e+02, 1.20000e-01, 0.00000e+00, 1.13351e+03,\n        1.10000e+00],\n       [1.83330e+02, 2.82330e+02, 5.50000e-01, 0.00000e+00, 6.24730e+02,\n        1.04000e+00],\n       [1.01466e+03, 1.26466e+03, 5.07000e+00, 8.14660e+02, 0.00000e+00,\n        0.00000e+00],\n       [9.06600e+01, 1.34000e+02, 3.00000e-01, 0.00000e+00, 1.50000e-01,\n        0.00000e+00],\n       [2.00660e+02, 9.83300e+01, 3.30000e-01, 0.00000e+00, 6.81540e+02,\n        5.10000e-01],\n       [5.40660e+02, 5.58660e+02, 1.08000e+00, 6.20000e+01, 2.71000e+00,\n        9.00000e-02],\n       [8.00000e+01, 6.06600e+01, 1.30000e-01, 0.00000e+00, 9.10190e+02,\n        8.80000e-01],\n       [5.30660e+02, 2.81330e+02, 8.80000e-01, 3.60000e+01, 7.43210e+02,\n        7.20000e-01],\n       [1.66000e+02, 1.33000e+02, 1.30000e-01, 0.00000e+00, 2.46880e+02,\n        2.05000e+00],\n       [3.77660e+02, 3.10330e+02, 5.70000e-01, 0.00000e+00, 1.02890e+02,\n        5.70000e-01],\n       [1.43330e+02, 7.30000e+01, 2.30000e-01, 0.00000e+00, 1.03940e+02,\n        1.00000e-01],\n       [3.94660e+02, 4.73660e+02, 5.60000e-01, 0.00000e+00, 1.06000e+00,\n        3.00000e-02],\n       [5.35660e+02, 4.47330e+02, 4.40000e-01, 0.00000e+00, 1.05900e+01,\n        8.00000e-02],\n       [5.26600e+01, 5.66600e+01, 5.20000e-01, 0.00000e+00, 0.00000e+00,\n        0.00000e+00],\n       [1.38166e+03, 7.60660e+02, 2.30000e+00, 7.81660e+02, 2.48710e+02,\n        1.30000e-01],\n       [4.43300e+01, 4.23300e+01, 7.00000e-02, 0.00000e+00, 6.60000e-01,\n        0.00000e+00],\n       [7.16600e+01, 6.26600e+01, 1.10000e-01, 0.00000e+00, 5.35260e+02,\n        5.20000e-01],\n       [1.48330e+02, 5.66600e+01, 2.40000e-01, 0.00000e+00, 1.73830e+02,\n        1.60000e-01]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_arr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "data_arr = data_arr.T"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(\n",
    "    n_estimators=64,\n",
    "    random_state=0\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
